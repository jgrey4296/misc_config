:Law_of_Total_Probability:
Summing over a partition over a variable can give the independent probability.

P(A) = P(A, B₁) + P(A, B₂) ... + P(A, B_n)

Or with explicit probabilities:
P(A) = P(A|B₁)P(B₁) + P(A|B₂)P(B₂) ... + P(A|B_n)P(B_n)
:END:


:Bayes_Theorem:
P(A | B) = (P(B | A) * P(A)) / P(B)
:END:

:General_Formulas:
Conditional Probability                 -  P(A | B) = P(A ∩ B) / P(B)
Independence                            -  P(A)     = P(A|B) = P(A ∩ B) = P(A)P(B)
Conditional Independence                -  P(A|B,C) = P(A|C)
:END:
:Expectation_μ:
Also: population mean. µ.

Discrete:
    μ = E(X) = Σ x*P(X=x)

    Where P(X) is uniform, it becomes the arithmetic mean:
    u = E(X) = 1/n * Σx

Continuous:
E(X) = ∫ x P(X) dx

Conditional Expectation
E(Y|x) = Σ y P(Y=y|x)

Linear Functions:
If Y  = a   X + b then
 E(Y) = a E(X)

:END:
:Central_Moments:
nth moment = E((X-μ)ⁿ)

Odd moments are 0 for symmetric distributions.
:END:
:Variance:
σ²(X)
Var(X) = E((X - E(X))²)

Var(X) = E(X²) - E(X)²

When Y = a + bX
Var(Y) = b² * Var(X)

or
Var(X) = Cov(X,X)
Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)

Product of Independent Variables:
Var(XY) = (E(X²)E(Y²)) - (E(X)²E(Y)²)
Product of Dependent Variables:
Var(XY) = E(X²Y²) - E(XY)²
:END:
:StandardDeviation:
σ = sqrt(Var(X))
σ = sqrt(Cov(X,X))

Expressed in the same units as X.
:END:
:Covariance:
σ(XY) = E( (X  - E(X)) (Y - E(Y)) )
σ(XY) = E(XY) - (E(X) * E(Y))
σ(XY) = ΣXY   - (ΣX * ΣY)

Measures association between two variables.

Properties:
Cov(aX, bY)   = a * b * Cov(X, Y)
Cov(X + c, Y) = Cov(X, Y)
Cov(X + Y, Z) = Cov(X, Z) + Cov(Y, Z)
Cov(X, X)     = Var(X)
Cov(X,Y)      = E(XY) - E(X)E(Y)

Cov(Y, X₁) = Cov(β₀ + β₁ * X₁ + β₂ * X₂, X₁) = β₁Var(X₁) + β₂Cov(X₂, X₁)
:END:
:Correlation:
Correlation_Coefficient is the normalization of covariance:
ρ(XY) = σ(XY) / (σ(X)σ(Y))
:END:
:Linear_Regression:
Linear Approximation of the line:
y = α + βx

Least Squares minimization of error:
(where y' = actual data value)
min(Σ(y_i - y'_i)²) = min(Σ(y_i - α - βx_i)²)

where β = R(YX) = σ(XY) / σ²(X)
then  α = E(Y) - β*E(X)

:END:
:Multiple_linear_regression:
Using the line:
y = α + (β₁*X₁) + (β₂*X₂)... + ε

By partially regressing on each variable, holding the others steady.
R(X₁Y.X₂) = Partial Regression Coefficient.

and minimizing the covariance between each variable and the error ε.
Cov(ε, Xᵢ)

giving:
denom = Var(X₁)Var(X₂) - Cov(X₁X₂)²
β₁ = R(YX₁.X₂) = ( Var(X₂)Cov(YX₁) - Cov(YX₂)Cov(X₂X₁) ) / denom

β₂ = R(YX₂.X₁) = ( σ²(X₂)σ(YX₂)σ(YX₂) - σ(YX₁)σ(YX₂) ) / denom

α  = E(Y)
:END:

Cosine Similarity - n-dimensional vector similarity using (A . B) / (||A|| x ||B||) -> cos θ
Distribution                            -
Joint Distribution                      -
Partition                               - Exhaustive, mutually exclusive set of events

Marginalization                         - Summing all probabilities of a variable.
Conditioning                            -
Conjugate Prior                         -
Central Limit Theorem -
